{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMCS3003 - Distributed Systems and Parallel Computing\n",
    "# Chapter 1: Introduction to Distributed Systems\n",
    "\n",
    "**Course:** Distributed Systems and Parallel Computing  \n",
    "**Instructor:** Assoc Prof Ts Dr Tew Yiqi  \n",
    "**Focus:** Parallel Processing and Distributed Systems Fundamentals\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [What is a Distributed System?](#1-what-is-a-distributed-system)\n",
    "2. [Traditional vs Distributed Systems](#2-traditional-vs-distributed-systems)\n",
    "3. [Benefits of Distributed Systems](#3-benefits-of-distributed-systems)\n",
    "4. [Challenges in Building Distributed Systems](#4-challenges-in-building-distributed-systems)\n",
    "5. [Hardware Architectures: Tightly vs Loosely Coupled](#5-hardware-architectures)\n",
    "6. [Transparency in Distributed Systems](#6-transparency-in-distributed-systems)\n",
    "7. [Real-time Systems](#7-real-time-systems)\n",
    "8. [Clock Synchronization](#8-clock-synchronization)\n",
    "9. [Operating Systems for Distributed Processing](#9-operating-systems)\n",
    "10. [Practical Examples and Exercises](#10-practical-examples)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Distributed System?\n",
    "\n",
    "### Definition\n",
    "\n",
    "A **distributed system** is a computing environment in which various components are spread across multiple computers (or nodes) that work together and coordinate their actions by passing messages through a network.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Multiple Processing Entities**: Computation is distributed/spread across multiple processing entities\n",
    "- **Network Communication**: Components communicate via network protocols\n",
    "- **Shared Goal**: Components work together to achieve a common objective\n",
    "- **Appears as Single System**: To end-users, it appears as a single coherent system\n",
    "\n",
    "### What Can Be Distributed?\n",
    "\n",
    "Various aspects of the system can be distributed:\n",
    "\n",
    "1. **Database/Data**: Distributed across multiple locations\n",
    "2. **Operating System**: Distributed OS manages resources globally\n",
    "3. **File System**: Files stored across multiple servers\n",
    "4. **Business Logic**: Application logic distributed across nodes\n",
    "5. **Authentication**: Centralized or distributed authentication services\n",
    "6. **Workload**: Resources allocation and load sharing\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "- **Web Services**: Google, Facebook, Amazon\n",
    "- **Cloud Computing**: AWS, Azure, Google Cloud\n",
    "- **Blockchain**: Bitcoin, Ethereum\n",
    "- **Content Delivery Networks (CDN)**: Cloudflare, Akamai\n",
    "- **Distributed Databases**: MongoDB, Cassandra, DynamoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional vs Distributed Systems\n",
    "\n",
    "### Traditional Databases (1990s to 2000s)\n",
    "\n",
    "- Single centralized database server\n",
    "- Backup to media server or tape/disk\n",
    "- Limited scalability (vertical scaling only)\n",
    "- Single point of failure\n",
    "\n",
    "### Distributed Databases (2010s and Beyond)\n",
    "\n",
    "- Multiple database nodes interconnected\n",
    "- **Scale-Out vs Scale-Up**: Horizontal scaling by adding more machines\n",
    "- **Local Storage vs Shared Storage**: Data can be partitioned or replicated\n",
    "- **Elastic vs Static Infrastructure**: Resources can be dynamically allocated\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Aspect | Traditional System | Distributed System |\n",
    "|--------|-------------------|--------------------|\n",
    "| Architecture | Centralized | Decentralized |\n",
    "| Scalability | Vertical (Scale-Up) | Horizontal (Scale-Out) |\n",
    "| Fault Tolerance | Low (Single Point of Failure) | High (Redundancy) |\n",
    "| Cost | High (Specialized Hardware) | Lower (Commodity Hardware) |\n",
    "| Complexity | Lower | Higher |\n",
    "| Performance | Limited by single machine | Can handle massive workloads |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benefits of Distributed Systems\n",
    "\n",
    "### 3.1 Scalability\n",
    "\n",
    "**Definition**: Ability to continuously evolve to support growing amounts of work.\n",
    "\n",
    "**Types of Scalability**:\n",
    "- **Horizontal Scalability**: Adding more machines (becomes efficient after a certain point)\n",
    "- **Vertical Scalability**: Upgrading existing machine (costs rise sharply after a certain point)\n",
    "\n",
    "**Key Points**:\n",
    "- Initial costs for horizontal scalability tend to be higher\n",
    "- Horizontal scalability becomes much more efficient after a certain point\n",
    "- Easier to add/remove nodes dynamically\n",
    "\n",
    "### 3.2 Reliability\n",
    "\n",
    "**Definition**: Ability to keep delivering services even when one or several software/hardware components fail.\n",
    "\n",
    "**Mechanisms**:\n",
    "- **Replication**: Multiple copies of data/services\n",
    "- **Redundancy**: Backup components ready to take over\n",
    "- **Failover**: Automatic switching to backup systems\n",
    "\n",
    "**Example**: Facebook's Maelstrom system - handles failures gracefully with resource pools and failover mechanisms\n",
    "\n",
    "### 3.3 Performance\n",
    "\n",
    "**Definition**: Ability to interact and coordinate actions to appear to the end-user as a single system.\n",
    "\n",
    "**Advantages**:\n",
    "- Parallel processing of tasks\n",
    "- Load balancing across nodes\n",
    "- Reduced latency through geographical distribution\n",
    "- Better resource utilization\n",
    "\n",
    "### 3.4 Geographical Distribution\n",
    "\n",
    "**Definition**: Distributed processing and resources based on application-specific requirements and user locality.\n",
    "\n",
    "**Benefits**:\n",
    "- Reduced latency for users in different regions\n",
    "- Compliance with data sovereignty regulations\n",
    "- Better disaster recovery options\n",
    "- Optimized for local requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Challenges in Building Distributed Systems\n",
    "\n",
    "### Major Challenges\n",
    "\n",
    "1. **Avoiding Single Point of Failure**\n",
    "   - Design redundancy into the system\n",
    "   - Use replication and failover mechanisms\n",
    "   - Monitor health of all components\n",
    "\n",
    "2. **Replication**\n",
    "   - Maintaining consistency across replicas\n",
    "   - Dealing with network partitions\n",
    "   - Choosing appropriate replication strategy\n",
    "\n",
    "3. **Availability and Performance**\n",
    "   - Balancing availability with consistency\n",
    "   - Managing network latency\n",
    "   - Optimizing data access patterns\n",
    "\n",
    "4. **Resource Naming, Addressing, and Location**\n",
    "   - DNS and service discovery\n",
    "   - Dynamic IP addresses\n",
    "   - Location transparency\n",
    "\n",
    "5. **Binding**\n",
    "   - Mapping between different parts of the system\n",
    "   - Dynamic binding and late binding\n",
    "   - Service registry and discovery\n",
    "\n",
    "### The CAP Theorem\n",
    "\n",
    "In distributed systems, you can only guarantee 2 out of 3:\n",
    "\n",
    "- **Consistency**: All nodes see the same data at the same time\n",
    "- **Availability**: Every request receives a response\n",
    "- **Partition Tolerance**: System continues to operate despite network failures\n",
    "\n",
    "**Note**: In practice, partition tolerance is a must, so you choose between consistency and availability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hardware Architectures\n",
    "\n",
    "### 5.1 Tightly-Coupled Systems (Parallel Processing Systems)\n",
    "\n",
    "**Characteristics**:\n",
    "- Processor units are physically part of the same computer\n",
    "- Connected by high-speed backplane bus or on same motherboard/chip\n",
    "- **Shared clock** - synchronization is possible\n",
    "- **Shared memory** - fast and reliable inter-processor communication\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "┌─────────────────────────────────────────────┐\n",
    "│              Shared Clock                    │\n",
    "└─────────────────────────────────────────────┘\n",
    "    ┌─────┐     ┌─────┐     ┌─────┐\n",
    "    │ CPU │     │ CPU │     │ CPU │\n",
    "    └──┬──┘     └──┬──┘     └──┬──┘\n",
    "    ┌──┴──┐     ┌──┴──┐     ┌──┴──┐\n",
    "    │Cache│     │Cache│     │Cache│\n",
    "    └──┬──┘     └──┬──┘     └──┬──┘\n",
    "┌──────┴─────────┴───────────┴──────┐\n",
    "│        Shared Memory               │\n",
    "└────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Hardware Features**:\n",
    "- **Specialized hardware**: Fixed architecture (number of processors)\n",
    "- **Expensive**: High-end processors and specialized interconnects\n",
    "- **Multi-core**: Typically 2-64 cores in modern systems\n",
    "- **Limited scale**: Large scale (>64 processors) not common\n",
    "\n",
    "**Examples**: Multi-core CPUs (Intel Xeon, AMD EPYC), GPUs\n",
    "\n",
    "### 5.2 Loosely-Coupled Systems (Distributed Systems)\n",
    "\n",
    "**Characteristics**:\n",
    "- Processor units are within separated computers\n",
    "- Connected by network technology (Ethernet, WiFi, etc.)\n",
    "- **Separate clocks** - absolute synchronization NOT possible\n",
    "- **Separate memory** - communication via message passing\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "┌──────────┐  ┌──────────┐  ┌──────────┐\n",
    "│  Cache   │  │  Cache   │  │  Cache   │\n",
    "│┌────────┐│  │┌────────┐│  │┌────────┐│\n",
    "││  CPU   ││  ││  CPU   ││  ││  CPU   ││\n",
    "│└────────┘│  │└────────┘│  │└────────┘│\n",
    "│  Clock 1 │  │  Clock 2 │  │  Clock 3 │\n",
    "└────┬─────┘  └────┬─────┘  └────┬─────┘\n",
    "     │             │             │\n",
    "  ───┴─────────────┴─────────────┴───\n",
    "          Network (Ethernet)\n",
    "```\n",
    "\n",
    "**Hardware Features**:\n",
    "- **General purpose hardware**: Cheap commodity servers\n",
    "- **Abundant**: Easy to scale by adding more machines\n",
    "- **Heterogeneous**: Different memory, disk, processor speed, OS\n",
    "- **Autonomous**: Each computer operates independently\n",
    "\n",
    "**Challenges**:\n",
    "- Each computer has its own clock (loose synchronization needed)\n",
    "- Separate memory (not suitable for direct inter-processor communication)\n",
    "- Autonomous and heterogeneous (need overall coordination)\n",
    "\n",
    "**Examples**: Data centers, cloud computing clusters, Hadoop clusters\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Feature | Tightly-Coupled | Loosely-Coupled |\n",
    "|---------|-----------------|------------------|\n",
    "| Communication | Shared Memory | Message Passing |\n",
    "| Synchronization | Shared Clock | Network Protocols |\n",
    "| Cost | High | Low |\n",
    "| Scalability | Limited | Excellent |\n",
    "| Programming | Easier (Shared Memory) | Harder (Distributed) |\n",
    "| Fault Tolerance | Lower | Higher |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transparency in Distributed Systems\n",
    "\n",
    "### What is Transparency?\n",
    "\n",
    "**Transparency** means hiding the details of distribution from users and application programmers.\n",
    "\n",
    "**Goal**: Reduce the burden on developers so they can focus on business logic rather than dealing with technical issues of distribution.\n",
    "\n",
    "### Types of Transparency\n",
    "\n",
    "#### 6.1 Access Transparency\n",
    "- Local and remote objects accessed with the same operations\n",
    "- Users don't need to know whether resource is local or remote\n",
    "- **Example**: Same API for local and remote file access\n",
    "\n",
    "#### 6.2 Location Transparency\n",
    "- Objects can be accessed without knowledge of their location\n",
    "- Resource names don't reveal physical location\n",
    "- **Example**: URL doesn't reveal which server hosts the resource\n",
    "\n",
    "#### 6.3 Concurrency Transparency\n",
    "- Concurrent processes can use shared objects without interference\n",
    "- System handles synchronization automatically\n",
    "- **Example**: Database transactions with ACID properties\n",
    "\n",
    "#### 6.4 Replication Transparency\n",
    "- Multiple copies of objects can exist without users knowing\n",
    "- System manages replicas automatically\n",
    "- **Example**: CDN caching content in multiple locations\n",
    "\n",
    "#### 6.5 Failure Transparency\n",
    "- Faults are concealed; applications continue without knowledge of failures\n",
    "- Automatic recovery and retry mechanisms\n",
    "- **Example**: Automatic failover in database clusters\n",
    "\n",
    "#### 6.6 Migration Transparency\n",
    "- **For Data**: Objects can be moved without affecting operations\n",
    "- **For Processes**: Processes can be moved without affecting results\n",
    "- **Example**: Virtual machine migration in cloud computing\n",
    "\n",
    "#### 6.7 Performance Transparency\n",
    "- Performance degrades gracefully as load increases\n",
    "- System adapts to varying workloads\n",
    "- **Example**: Auto-scaling in cloud platforms\n",
    "\n",
    "#### 6.8 Scaling Transparency\n",
    "- System can scale without changing structure or algorithms\n",
    "- Same application code works on 10 or 10,000 nodes\n",
    "- **Example**: Stateless microservices that can be replicated\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- Complete transparency is often impossible or undesirable\n",
    "- May hide performance problems\n",
    "- Network delays and failures cannot always be hidden\n",
    "- Trade-off between transparency and control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-time Systems\n",
    "\n",
    "### Definition\n",
    "\n",
    "A **real-time system** is any information processing system with hardware and software components that perform real-time application functions and can respond to events within predictable and specific time constraints.\n",
    "\n",
    "### Components\n",
    "\n",
    "- **Controlling System**: Computer that processes inputs and generates outputs\n",
    "- **Controlled System**: Environment being monitored/controlled\n",
    "- **Sensors**: Collect data from environment\n",
    "- **Actuators**: Perform actions in environment\n",
    "\n",
    "### Typical Features\n",
    "\n",
    "1. **Time Critical**: Deadlines must be met\n",
    "2. **Concurrent Processes**: Multiple tasks running simultaneously\n",
    "3. **Resource Sharing**: Tasks share CPU, memory, I/O\n",
    "4. **Inter-process Communication**: Tasks coordinate and share data\n",
    "5. **Reliability and Fault Tolerance**: Essential for safety-critical systems\n",
    "6. **Specific Purpose**: Dedicated to particular functions\n",
    "\n",
    "### Real-World Examples\n",
    "\n",
    "1. **Air Traffic Control Systems**\n",
    "   - Monitor aircraft positions\n",
    "   - Prevent collisions\n",
    "   - Time-critical decisions\n",
    "\n",
    "2. **Autonomous Vehicles**\n",
    "   - Sensor fusion\n",
    "   - Real-time path planning\n",
    "   - Immediate response to obstacles\n",
    "\n",
    "3. **Medical Devices**\n",
    "   - Heart monitors\n",
    "   - Insulin pumps\n",
    "   - Life-critical timing\n",
    "\n",
    "4. **Industrial Control**\n",
    "   - Manufacturing robots\n",
    "   - Process control\n",
    "   - Precise timing requirements\n",
    "\n",
    "### Types of Real-Time Systems\n",
    "\n",
    "#### Hard Real-Time\n",
    "- Missing deadline is catastrophic\n",
    "- **Example**: Aircraft control, medical devices\n",
    "\n",
    "#### Soft Real-Time\n",
    "- Missing deadline degrades performance but not catastrophic\n",
    "- **Example**: Video streaming, online gaming\n",
    "\n",
    "### Distributed Real-Time Systems\n",
    "\n",
    "Real-time systems are often implemented as distributed systems for:\n",
    "\n",
    "1. **Fault Tolerance**: Redundancy for safety\n",
    "2. **Geographical Requirements**: Sensors/actuators at different locations\n",
    "3. **Performance**: Parallel processing for meeting deadlines\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "┌────────┐   ┌────────┐   ┌────────┐\n",
    "│ Node A │   │ Node B │   │ Node C │\n",
    "└───┬────┘   └───┬────┘   └───┬────┘\n",
    "    └────────────┼────────────┘\n",
    "    Real-time Communication Network\n",
    "         (Deterministic Timing)\n",
    "    ┌────────────┼────────────┐\n",
    "┌───┴────┐   ┌───┴────┐   ┌───┴────┐\n",
    "│ Node D │   │ Node E │   │ Node F │\n",
    "└────────┘   └────────┘   └────────┘\n",
    "```\n",
    "\n",
    "### Specific Issues in Distributed Real-Time Systems\n",
    "\n",
    "1. **Clock Synchronization**: Critical for coordinated actions\n",
    "2. **Real-time Communication**: Guaranteed message delivery times\n",
    "3. **Distributed Scheduling**: Allocating tasks to processors\n",
    "4. **Fault Tolerance**: Handling failures without missing deadlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Clock Synchronization\n",
    "\n",
    "### Why Clock Synchronization?\n",
    "\n",
    "In distributed systems:\n",
    "- Each computer has its own clock\n",
    "- Clocks **drift** over time (gain or lose time)\n",
    "- Different clocks have **skew** (difference between them)\n",
    "- Need coordination for distributed actions\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Clock Drift**: The process of gaining/losing time relative to perfect reference clock (UTC)\n",
    "- **Clock Skew**: The difference between two clocks at a given point in time\n",
    "- **Clock Synchronization**: Aims to minimize clock skew between clocks\n",
    "\n",
    "### 8.1 Cristian's Algorithm (1989)\n",
    "\n",
    "#### Overview\n",
    "- Uses a time server synchronized to UTC\n",
    "- Clients request time from server\n",
    "- Attempts to compensate for network delays\n",
    "\n",
    "#### Protocol\n",
    "\n",
    "1. Client notes local time T₀ before sending request\n",
    "2. Server receives request, sends back current time Tₛ\n",
    "3. Client notes local time T₁ when receiving reply\n",
    "4. Client estimates correct time as: **Tₛ + (T₁ - T₀) / 2**\n",
    "\n",
    "#### Assumptions\n",
    "- Symmetric network delays (same in both directions)\n",
    "- Processing time is negligible or known\n",
    "\n",
    "#### Problems\n",
    "1. Network delays vary over time\n",
    "2. Network delays can differ in each direction\n",
    "3. Processing delays also vary\n",
    "\n",
    "#### Example Calculation\n",
    "```\n",
    "T₀ = 08:02:01.670 (client sends request)\n",
    "Tₛ = 08:02:04.325 (server time)\n",
    "T₁ = 08:02:02.130 (client receives response)\n",
    "\n",
    "RTT = T₁ - T₀ = 460ms\n",
    "One-way delay ≈ 230ms\n",
    "\n",
    "Correct time ≈ 08:02:04.325 + 0.230 = 08:02:04.555\n",
    "Client needs to gain: 08:02:04.555 - 08:02:02.130 = 2.425 seconds\n",
    "```\n",
    "\n",
    "### 8.2 Berkeley Algorithm (1989)\n",
    "\n",
    "#### Overview\n",
    "- Master-slave architecture\n",
    "- Master doesn't need to be synchronized to UTC\n",
    "- Computes average time across all nodes\n",
    "\n",
    "#### Protocol\n",
    "\n",
    "1. Master sends time requests to all slaves\n",
    "2. Slaves respond with their local times\n",
    "3. Master computes average (excluding outliers)\n",
    "4. Master sends adjustment values to each node\n",
    "\n",
    "#### Example\n",
    "```\n",
    "Master M: 08:01:17\n",
    "Slave A:  08:01:12 (difference: -5 seconds)\n",
    "Slave B:  08:02:01 (difference: +44 seconds)\n",
    "Slave C:  12:05:21 (outlier - ignored)\n",
    "\n",
    "Average = (08:01:17 + 08:01:12 + 08:02:01) / 3 = 01:30\n",
    "\n",
    "Adjustments sent:\n",
    "M: +00:00:13\n",
    "A: +00:00:18  \n",
    "B: -00:00:31\n",
    "C: -03:43:01 (outlier, large adjustment)\n",
    "```\n",
    "\n",
    "#### Advantages\n",
    "- No external time source needed\n",
    "- Handles outliers\n",
    "- Provides internal consistency\n",
    "\n",
    "#### Disadvantages\n",
    "- Master is single point of failure\n",
    "- Not synchronized to external time standard\n",
    "- More network traffic than Cristian's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstration: Clock Synchronization Simulation\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class Clock:\n",
    "    \"\"\"Simulates a clock that can drift\"\"\"\n",
    "    def __init__(self, name, initial_time, drift_rate=0.0):\n",
    "        self.name = name\n",
    "        self.time = initial_time\n",
    "        self.drift_rate = drift_rate  # seconds per second\n",
    "        \n",
    "    def tick(self, seconds=1.0):\n",
    "        \"\"\"Advance clock by given seconds (with drift)\"\"\"\n",
    "        self.time += timedelta(seconds=seconds * (1 + self.drift_rate))\n",
    "        \n",
    "    def get_time(self):\n",
    "        return self.time\n",
    "    \n",
    "    def set_time(self, new_time):\n",
    "        self.time = new_time\n",
    "        \n",
    "    def adjust_time(self, adjustment):\n",
    "        \"\"\"Adjust clock by given timedelta\"\"\"\n",
    "        self.time += adjustment\n",
    "\n",
    "def simulate_network_delay():\n",
    "    \"\"\"Simulate variable network delay in seconds\"\"\"\n",
    "    return random.uniform(0.01, 0.1)  # 10-100ms\n",
    "\n",
    "def cristian_algorithm_demo():\n",
    "    \"\"\"Demonstrate Cristian's algorithm\"\"\"\n",
    "    print(\"=== Cristian's Algorithm Demo ===\")\n",
    "    print()\n",
    "    \n",
    "    # Create server and client clocks\n",
    "    base_time = datetime.now()\n",
    "    server = Clock(\"Server\", base_time, drift_rate=0.0)  # No drift (UTC source)\n",
    "    client = Clock(\"Client\", base_time - timedelta(seconds=5), drift_rate=0.001)  # 5 sec behind, slight drift\n",
    "    \n",
    "    print(f\"Initial State:\")\n",
    "    print(f\"Server time: {server.get_time().strftime('%H:%M:%S.%f')[:-3]}\")\n",
    "    print(f\"Client time: {client.get_time().strftime('%H:%M:%S.%f')[:-3]}\")\n",
    "    print(f\"Skew: {(server.get_time() - client.get_time()).total_seconds():.3f} seconds\")\n",
    "    print()\n",
    "    \n",
    "    # Client synchronization\n",
    "    print(\"Client initiating synchronization...\")\n",
    "    T0 = client.get_time()\n",
    "    print(f\"T₀ (request sent): {T0.strftime('%H:%M:%S.%f')[:-3]}\")\n",
    "    \n",
    "    # Simulate network delay to server\n",
    "    delay1 = simulate_network_delay()\n",
    "    time.sleep(delay1)\n",
    "    \n",
    "    # Server responds with its time\n",
    "    Ts = server.get_time()\n",
    "    print(f\"Tₛ (server time):  {Ts.strftime('%H:%M:%S.%f')[:-3]}\")\n",
    "    \n",
    "    # Simulate network delay from server\n",
    "    delay2 = simulate_network_delay()\n",
    "    time.sleep(delay2)\n",
    "    \n",
    "    T1 = client.get_time()\n",
    "    print(f\"T₁ (reply received): {T1.strftime('%H:%M:%S.%f')[:-3]}\")\n",
    "    \n",
    "    # Calculate adjustment\n",
    "    RTT = (T1 - T0).total_seconds()\n",
    "    estimated_current_time = Ts + timedelta(seconds=RTT/2)\n",
    "    adjustment = estimated_current_time - T1\n",
    "    \n",
    "    print(f\"\\nRound Trip Time: {RTT*1000:.1f} ms\")\n",
    "    print(f\"Estimated one-way delay: {RTT*1000/2:.1f} ms\")\n",
    "    print(f\"Adjustment needed: {adjustment.total_seconds():.3f} seconds\")\n",
    "    \n",
    "    # Apply adjustment\n",
    "    client.adjust_time(adjustment)\n",
    "    \n",
    "    print(f\"\\nAfter Synchronization:\")\n",
    "    print(f\"Server time: {server.get_time().strftime('%H:%M:%S.%f')[:-3]}\")\n",
    "    print(f\"Client time: {client.get_time().strftime('%H:%M:%S.%f')[:-3]}\")\n",
    "    print(f\"Remaining skew: {abs((server.get_time() - client.get_time()).total_seconds())*1000:.1f} ms\")\n",
    "\n",
    "# Run demonstration\n",
    "cristian_algorithm_demo()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def berkeley_algorithm_demo():\n",
    "    \"\"\"Demonstrate Berkeley algorithm\"\"\"\n",
    "    print(\"\\n=== Berkeley Algorithm Demo ===\")\n",
    "    print()\n",
    "    \n",
    "    # Create master and slave clocks\n",
    "    base_time = datetime.now()\n",
    "    master = Clock(\"Master\", base_time + timedelta(seconds=0))\n",
    "    slave1 = Clock(\"Slave1\", base_time - timedelta(seconds=5))\n",
    "    slave2 = Clock(\"Slave2\", base_time + timedelta(seconds=44))\n",
    "    slave3 = Clock(\"Slave3\", base_time + timedelta(hours=4, seconds=4))  # Outlier\n",
    "    \n",
    "    nodes = [master, slave1, slave2, slave3]\n",
    "    \n",
    "    print(\"Initial State:\")\n",
    "    for node in nodes:\n",
    "        print(f\"{node.name:8s}: {node.get_time().strftime('%H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    # Master collects times\n",
    "    print(\"Master collecting times from all nodes...\")\n",
    "    times = [node.get_time() for node in nodes]\n",
    "    \n",
    "    # Calculate differences from master\n",
    "    master_time = times[0]\n",
    "    differences = [(t - master_time).total_seconds() for t in times]\n",
    "    \n",
    "    print(\"\\nDifferences from Master:\")\n",
    "    for node, diff in zip(nodes, differences):\n",
    "        print(f\"{node.name:8s}: {diff:+7.0f} seconds\")\n",
    "    \n",
    "    # Remove outliers (more than 1 minute difference)\n",
    "    valid_times = [t for t, d in zip(times, differences) if abs(d) < 60]\n",
    "    \n",
    "    print(f\"\\nExcluding outliers (>{60}s difference)\")\n",
    "    print(f\"Valid nodes: {len(valid_times)}\")\n",
    "    \n",
    "    # Calculate average\n",
    "    avg_seconds = sum([t.timestamp() for t in valid_times]) / len(valid_times)\n",
    "    avg_time = datetime.fromtimestamp(avg_seconds)\n",
    "    \n",
    "    print(f\"Average time: {avg_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Calculate adjustments\n",
    "    print(\"\\nAdjustments to send:\")\n",
    "    for node in nodes:\n",
    "        adjustment = avg_time - node.get_time()\n",
    "        print(f\"{node.name:8s}: {adjustment.total_seconds():+7.0f} seconds\")\n",
    "        node.adjust_time(adjustment)\n",
    "    \n",
    "    print(\"\\nAfter Synchronization:\")\n",
    "    for node in nodes:\n",
    "        print(f\"{node.name:8s}: {node.get_time().strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Run demonstration\n",
    "berkeley_algorithm_demo()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Operating Systems for Distributed Processing\n",
    "\n",
    "### What Does an Operating System Do?\n",
    "\n",
    "An OS manages all software and hardware on computers:\n",
    "\n",
    "1. **Process/Thread Management**\n",
    "   - Scheduling\n",
    "   - Communication\n",
    "   - Synchronization\n",
    "\n",
    "2. **Memory Management**\n",
    "   - Allocation\n",
    "   - Virtual memory\n",
    "   - Paging\n",
    "\n",
    "3. **Storage Management**\n",
    "   - Disk scheduling\n",
    "   - Caching\n",
    "\n",
    "4. **File Systems Management**\n",
    "   - File organization\n",
    "   - Access control\n",
    "\n",
    "5. **Protection and Security**\n",
    "   - Authentication\n",
    "   - Authorization\n",
    "   - Encryption\n",
    "\n",
    "6. **Networking**\n",
    "   - Network protocols\n",
    "   - Socket management\n",
    "\n",
    "### Types of Distributed Operating Systems\n",
    "\n",
    "#### Network Operating System (NOS)\n",
    "\n",
    "**Examples**: Microsoft Windows Server, UNIX, Linux, Mac OS X\n",
    "\n",
    "**Characteristics**:\n",
    "- Each computer runs its own OS\n",
    "- Users aware of network\n",
    "- Explicit network operations (remote login, file transfer)\n",
    "- Low degree of transparency\n",
    "- High autonomy\n",
    "- Communication via files\n",
    "- Per-node resource management\n",
    "- Highly scalable\n",
    "- Open systems\n",
    "\n",
    "#### Distributed Operating System (DOS)\n",
    "\n",
    "**Examples**: Solaris, Mach, Micros\n",
    "\n",
    "**Types**:\n",
    "1. **Multiprocessor DOS**\n",
    "   - One OS image for all processors\n",
    "   - Shared memory communication\n",
    "   - Global, centralized resource management\n",
    "   - Very high transparency\n",
    "   - Not scalable\n",
    "   - Closed system\n",
    "\n",
    "2. **Multicomputer DOS**\n",
    "   - Same OS on all nodes (N copies)\n",
    "   - Message-based communication\n",
    "   - Global, distributed resource management\n",
    "   - High transparency\n",
    "   - Moderately scalable\n",
    "   - Closed system\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Feature | Multiprocessor DOS | Multicomputer DOS | Network OS |\n",
    "|---------|-------------------|-------------------|------------|\n",
    "| Transparency | Very High | High | Low |\n",
    "| Same OS on all nodes | Yes | Yes | No |\n",
    "| Number of OS copies | 1 | N | N |\n",
    "| Communication | Shared memory | Messages | Files |\n",
    "| Resource Management | Global, central | Global, distributed | Per node |\n",
    "| Scalability | No | Moderately | Yes |\n",
    "| Openness | Closed | Closed | Open |\n",
    "\n",
    "### Modern Trends\n",
    "\n",
    "- **Containers**: Docker, Kubernetes (application-level distribution)\n",
    "- **Serverless**: AWS Lambda, Azure Functions\n",
    "- **Microservices**: Independent services communicating via APIs\n",
    "- **Edge Computing**: Processing at network edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practical Examples and Exercises\n",
    "\n",
    "### Example 1: Simple Parallel Processing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Parallel vs Sequential Processing\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def cpu_intensive_task(n):\n",
    "    \"\"\"Simulate CPU-intensive work\"\"\"\n",
    "    result = 0\n",
    "    for i in range(n):\n",
    "        result += i ** 2\n",
    "    return result\n",
    "\n",
    "def sequential_processing():\n",
    "    \"\"\"Process tasks sequentially\"\"\"\n",
    "    print(\"Sequential Processing:\")\n",
    "    start = time.time()\n",
    "    \n",
    "    tasks = [10000000] * 4\n",
    "    results = [cpu_intensive_task(n) for n in tasks]\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Time taken: {end - start:.2f} seconds\")\n",
    "    return results\n",
    "\n",
    "def parallel_processing():\n",
    "    \"\"\"Process tasks in parallel\"\"\"\n",
    "    print(\"\\nParallel Processing:\")\n",
    "    start = time.time()\n",
    "    \n",
    "    tasks = [10000000] * 4\n",
    "    with Pool(processes=4) as pool:\n",
    "        results = pool.map(cpu_intensive_task, tasks)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Time taken: {end - start:.2f} seconds\")\n",
    "    return results\n",
    "\n",
    "# Compare performance\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Number of CPU cores: {mp.cpu_count()}\")\n",
    "    print()\n",
    "    \n",
    "    seq_results = sequential_processing()\n",
    "    par_results = parallel_processing()\n",
    "    \n",
    "    print(f\"\\nResults match: {seq_results == par_results}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Understanding OpenMP (from Activity in Slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Python simulation of the OpenMP example from the slides\n",
    "# The original C++ code used #pragma omp parallel\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "def openmp_simulation():\n",
    "    \"\"\"Simulates the OpenMP parallel example\"\"\"\n",
    "    print(\"OpenMP Parallel Simulation\")\n",
    "    print(\"Original C++ code:\")\n",
    "    print(\"\"\"#pragma omp parallel\n",
    "{\n",
    "    cout << \"Hello World\\\\n\";\n",
    "}\"\"\")\n",
    "    print(\"\\nWhat this does:\")\n",
    "    print(\"- Creates multiple threads\")\n",
    "    print(\"- Each thread executes the code block\")\n",
    "    print(\"- Output 'Hello World' from each thread\\n\")\n",
    "    \n",
    "    # Simulate with Python threads\n",
    "    def print_hello(thread_id):\n",
    "        print(f\"Hello World from thread {thread_id}\")\n",
    "    \n",
    "    num_threads = 4\n",
    "    print(f\"Simulating with {num_threads} threads:\\n\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        executor.map(print_hello, range(num_threads))\n",
    "    \n",
    "    print(\"\\nNote: Order may vary due to thread scheduling!\")\n",
    "\n",
    "openmp_simulation()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Demonstrating Distributed Systems Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulating network partition and the CAP theorem\n",
    "import random\n",
    "import time\n",
    "\n",
    "class DistributedNode:\n",
    "    \"\"\"Simulates a node in a distributed system\"\"\"\n",
    "    def __init__(self, node_id):\n",
    "        self.node_id = node_id\n",
    "        self.data = {}\n",
    "        self.is_available = True\n",
    "        \n",
    "    def write(self, key, value):\n",
    "        \"\"\"Write data to this node\"\"\"\n",
    "        if not self.is_available:\n",
    "            raise Exception(f\"Node {self.node_id} is unavailable\")\n",
    "        self.data[key] = value\n",
    "        print(f\"Node {self.node_id}: Wrote {key}={value}\")\n",
    "        \n",
    "    def read(self, key):\n",
    "        \"\"\"Read data from this node\"\"\"\n",
    "        if not self.is_available:\n",
    "            raise Exception(f\"Node {self.node_id} is unavailable\")\n",
    "        return self.data.get(key, None)\n",
    "\n",
    "def simulate_cap_theorem():\n",
    "    \"\"\"Demonstrate CAP theorem trade-offs\"\"\"\n",
    "    print(\"=== CAP Theorem Demonstration ===\")\n",
    "    print(\"\\nCreating 3-node distributed system...\")\n",
    "    \n",
    "    nodes = [DistributedNode(i) for i in range(3)]\n",
    "    \n",
    "    # Scenario 1: Normal operation (CP - Consistency + Partition Tolerance)\n",
    "    print(\"\\n--- Scenario 1: Normal Operation ---\")\n",
    "    print(\"Writing 'user=alice' to all nodes...\")\n",
    "    for node in nodes:\n",
    "        node.write('user', 'alice')\n",
    "    \n",
    "    print(\"\\nReading from random node:\")\n",
    "    random_node = random.choice(nodes)\n",
    "    print(f\"Node {random_node.node_id}: user={random_node.read('user')}\")\n",
    "    print(\"✓ Consistency maintained\")\n",
    "    \n",
    "    # Scenario 2: Network partition (Choose between C and A)\n",
    "    print(\"\\n--- Scenario 2: Network Partition ---\")\n",
    "    print(\"Simulating network partition: Node 2 isolated\")\n",
    "    nodes[2].is_available = False\n",
    "    \n",
    "    print(\"\\nOption A: Prioritize Consistency (CP)\")\n",
    "    print(\"- Reject writes until all nodes available\")\n",
    "    print(\"- Result: System unavailable for writes\")\n",
    "    print(\"- Trade-off: Lost Availability\")\n",
    "    \n",
    "    print(\"\\nOption B: Prioritize Availability (AP)\")\n",
    "    print(\"- Allow writes to available nodes\")\n",
    "    nodes[0].write('user', 'bob')\n",
    "    nodes[1].write('user', 'bob')\n",
    "    print(\"- Node 2 still has old value (when it recovers)\")\n",
    "    print(\"- Result: Temporary inconsistency\")\n",
    "    print(\"- Trade-off: Lost Consistency\")\n",
    "    \n",
    "    print(\"\\n--- Conclusion ---\")\n",
    "    print(\"In presence of network Partition, must choose:\")\n",
    "    print(\"- Consistency (CP): Wait for all nodes, sacrifice availability\")\n",
    "    print(\"- Availability (AP): Serve requests, sacrifice consistency\")\n",
    "\n",
    "simulate_cap_theorem()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Clock Synchronization\n",
    "\n",
    "**Problem**: Given the following data for Cristian's algorithm, calculate the clock adjustment needed:\n",
    "\n",
    "- Client sends request at local time: 10:15:30.500\n",
    "- Server responds with time: 10:15:35.800\n",
    "- Client receives response at local time: 10:15:31.200\n",
    "\n",
    "Calculate:\n",
    "1. Round Trip Time (RTT)\n",
    "2. Estimated one-way delay\n",
    "3. Estimated correct time\n",
    "4. Required adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Solution\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def parse_time(time_str):\n",
    "    return datetime.strptime(time_str, \"%H:%M:%S.%f\")\n",
    "\n",
    "# Your code here\n",
    "T0 = parse_time(\"10:15:30.500\")\n",
    "Ts = parse_time(\"10:15:35.800\")\n",
    "T1 = parse_time(\"10:15:31.200\")\n",
    "\n",
    "# Calculate RTT\n",
    "RTT = (T1 - T0).total_seconds()\n",
    "print(f\"RTT: {RTT} seconds\")\n",
    "\n",
    "# Calculate one-way delay\n",
    "one_way_delay = RTT / 2\n",
    "print(f\"One-way delay: {one_way_delay} seconds\")\n",
    "\n",
    "# Calculate estimated correct time\n",
    "estimated_time = Ts + timedelta(seconds=one_way_delay)\n",
    "print(f\"Estimated correct time: {estimated_time.strftime('%H:%M:%S.%f')}\")\n",
    "\n",
    "# Calculate adjustment\n",
    "adjustment = (estimated_time - T1).total_seconds()\n",
    "print(f\"Adjustment needed: {adjustment} seconds\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Berkeley Algorithm\n",
    "\n",
    "**Problem**: Given a distributed system with following node times:\n",
    "\n",
    "- Master: 14:20:00\n",
    "- Slave1: 14:19:50\n",
    "- Slave2: 14:20:15\n",
    "- Slave3: 14:20:05\n",
    "\n",
    "Calculate the adjustment each node should make using Berkeley algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "# Parse times\n",
    "times = {\n",
    "    'Master': parse_time(\"14:20:00.000\"),\n",
    "    'Slave1': parse_time(\"14:19:50.000\"),\n",
    "    'Slave2': parse_time(\"14:20:15.000\"),\n",
    "    'Slave3': parse_time(\"14:20:05.000\")\n",
    "}\n",
    "\n",
    "# Calculate average\n",
    "avg_timestamp = sum([t.timestamp() for t in times.values()]) / len(times)\n",
    "avg_time = datetime.fromtimestamp(avg_timestamp)\n",
    "\n",
    "print(f\"Average time: {avg_time.strftime('%H:%M:%S')}\")\n",
    "print(\"\\nAdjustments:\")\n",
    "\n",
    "# Calculate adjustments\n",
    "for name, time in times.items():\n",
    "    adjustment = (avg_time - time).total_seconds()\n",
    "    print(f\"{name}: {adjustment:+.0f} seconds\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Parallel Processing Performance\n",
    "\n",
    "**Problem**: Write a function to calculate Fibonacci numbers and compare sequential vs parallel execution for calculating the first 35 Fibonacci numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "def fibonacci(n):\n",
    "    \"\"\"Calculate nth Fibonacci number (recursive - intentionally slow)\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\n",
    "def sequential_fibonacci(numbers):\n",
    "    \"\"\"Calculate Fibonacci numbers sequentially\"\"\"\n",
    "    start = time.time()\n",
    "    results = [fibonacci(n) for n in numbers]\n",
    "    elapsed = time.time() - start\n",
    "    return results, elapsed\n",
    "\n",
    "def parallel_fibonacci(numbers):\n",
    "    \"\"\"Calculate Fibonacci numbers in parallel\"\"\"\n",
    "    start = time.time()\n",
    "    with Pool(processes=mp.cpu_count()) as pool:\n",
    "        results = pool.map(fibonacci, numbers)\n",
    "    elapsed = time.time() - start\n",
    "    return results, elapsed\n",
    "\n",
    "# Test with numbers 30-34 (adjust based on your CPU speed)\n",
    "test_numbers = list(range(30, 35))\n",
    "\n",
    "print(\"Computing Fibonacci numbers...\")\n",
    "print(f\"Numbers to compute: {test_numbers}\")\n",
    "\n",
    "seq_results, seq_time = sequential_fibonacci(test_numbers)\n",
    "print(f\"\\nSequential: {seq_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    par_results, par_time = parallel_fibonacci(test_numbers)\n",
    "    print(f\"Parallel: {par_time:.2f} seconds\")\n",
    "    print(f\"\\nSpeedup: {seq_time/par_time:.2f}x\")\n",
    "    print(f\"Efficiency: {(seq_time/par_time)/mp.cpu_count()*100:.1f}%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Questions\n",
    "\n",
    "### Conceptual Questions\n",
    "\n",
    "1. **What is the main difference between a distributed system and a parallel system?**\n",
    "\n",
    "2. **Explain the CAP theorem and why you can only have 2 out of 3 properties.**\n",
    "\n",
    "3. **What are the advantages of loosely-coupled systems over tightly-coupled systems?**\n",
    "\n",
    "4. **Describe three types of transparency in distributed systems and give an example of each.**\n",
    "\n",
    "5. **Why is clock synchronization important in distributed real-time systems?**\n",
    "\n",
    "6. **Compare Cristian's Algorithm and Berkeley Algorithm for clock synchronization.**\n",
    "\n",
    "7. **What is the difference between Network OS and Distributed OS?**\n",
    "\n",
    "8. **Explain why hard real-time systems often need to be distributed.**\n",
    "\n",
    "### Practical Questions\n",
    "\n",
    "9. **Given a system with 4 nodes, if one node fails, how would you ensure:**\n",
    "   - Availability?\n",
    "   - Consistency?\n",
    "\n",
    "10. **Design a simple distributed system for an online shopping cart. What components would you distribute and why?**\n",
    "\n",
    "11. **If you have a task that takes 100 seconds on a single core, what would be the theoretical minimum time on:**\n",
    "    - 4 cores (parallel, tightly-coupled)?\n",
    "    - 4 machines (distributed, loosely-coupled)?\n",
    "    \n",
    "    What factors would make the actual time different from theoretical?\n",
    "\n",
    "12. **A distributed system has nodes with clocks showing: 10:00:00, 10:00:05, 09:59:58, 10:00:02. Using Berkeley algorithm, calculate the adjustments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Distributed Systems**: Multiple computers working together to appear as a single system\n",
    "\n",
    "2. **Benefits**: Scalability, Reliability, Performance, Geographical distribution\n",
    "\n",
    "3. **Challenges**: Single point of failure, replication, consistency, naming, binding\n",
    "\n",
    "4. **Architectures**:\n",
    "   - Tightly-coupled: Shared memory, shared clock, expensive, limited scale\n",
    "   - Loosely-coupled: Message passing, separate clocks, cheap, highly scalable\n",
    "\n",
    "5. **Transparency**: Hiding distribution complexity from users and developers\n",
    "\n",
    "6. **Real-time Systems**: Must meet timing deadlines, often distributed for reliability\n",
    "\n",
    "7. **Clock Synchronization**: Essential for coordinating distributed actions\n",
    "   - Cristian's Algorithm: Client-server, UTC source\n",
    "   - Berkeley Algorithm: Master-slave, average time\n",
    "\n",
    "8. **Operating Systems**:\n",
    "   - Network OS: High autonomy, low transparency\n",
    "   - Distributed OS: Low autonomy, high transparency\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In upcoming chapters, we will explore:\n",
    "- Inter-process communication mechanisms\n",
    "- Distributed algorithms and consensus\n",
    "- Parallel programming models (OpenMP, MPI)\n",
    "- Distributed databases and file systems\n",
    "- Cloud computing and virtualization\n",
    "- Microservices and containerization\n",
    "\n",
    "---\n",
    "\n",
    "## References and Further Reading\n",
    "\n",
    "1. Tanenbaum, A. S., & Van Steen, M. (2017). *Distributed Systems: Principles and Paradigms*. 3rd Edition.\n",
    "\n",
    "2. Coulouris, G., Dollimore, J., Kindberg, T., & Blair, G. (2011). *Distributed Systems: Concepts and Design*. 5th Edition.\n",
    "\n",
    "3. Cristian, F. (1989). \"Probabilistic clock synchronization\". *Distributed Computing*, 3(3), 146-158.\n",
    "\n",
    "4. Gusella, R., & Zatti, S. (1989). \"The accuracy of the clock synchronization achieved by TEMPO in Berkeley UNIX 4.3BSD\". *IEEE Transactions on Software Engineering*, 15(7), 847-853.\n",
    "\n",
    "5. Online Resources:\n",
    "   - [Distributed Systems Course - MIT](https://pdos.csail.mit.edu/6.824/)\n",
    "   - [CAP Theorem Explained](https://www.ibm.com/cloud/learn/cap-theorem)\n",
    "   - [Clock Synchronization Tutorial](https://www.cl.cam.ac.uk/teaching/)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 1**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
